{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0: Cost 7.260476429358543   \n",
      "Accuracy on training set: 14.6218487394958%\n",
      "Iteration  200: Cost 7.231812213021109   \n",
      "Accuracy on training set: 14.6218487394958%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pickle\n",
    "import math\n",
    "from featureSelection import features_selection\n",
    "\n",
    "with open('data_train.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "    \n",
    "features_list=[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18]\n",
    "X_train, X_val, y_train, y_val = features_selection(features_list)\n",
    "\n",
    "def sigmoid(Z):\n",
    "    return 1/(1+np.exp(-Z))\n",
    "\n",
    "\n",
    "def initialize_parameters(dim):\n",
    "    W = np.random.rand(dim, 1)\n",
    "    b = 0\n",
    "    return W, b\n",
    "\n",
    "W,b = initialize_parameters(X_train.shape[1])\n",
    "\n",
    "\n",
    "\n",
    "def compute_gradient(X, Y, W, b):\n",
    "    \n",
    "\n",
    "    m,n = X.shape\n",
    "    dj_dw = np.zeros((n,))                           #(n,)\n",
    "    dj_db = 0.\n",
    "    loss=0.\n",
    "    \n",
    "    \n",
    "    for i in range(m):\n",
    "        A=(np.dot(X[i],W)+b)\n",
    "       \n",
    "        loss+=Y[i]*np.log(sigmoid(A[0]))+(1-Y[i])*np.log(1-sigmoid(A[0]))\n",
    "        Z=sigmoid(A[0])\n",
    "        for j in range(n):\n",
    "            dj_dw[j]=dj_dw[j]+(Z-Y[i])*X[i,j]\n",
    "        dj_db+=(Z-Y[i])\n",
    "        dj_dw = dj_dw/m                                   #(n,)\n",
    "        dj_db = dj_db/m  \n",
    "        \n",
    "    cost=-loss/m\n",
    "        \n",
    "    \n",
    "    return cost,dj_dw, dj_db\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def predict(X_i, W,  b,threshold=0.5):\n",
    "    A=(np.dot(X_i,W)+b)\n",
    "    Z=  sigmoid(A[0])\n",
    "    if Z >threshold:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "\n",
    "def accuracy(X, y, W, b):\n",
    "    \n",
    "    m = X.shape[0]\n",
    "    correct = 0\n",
    "    for i in range(m):\n",
    "        if predict(X[i], W, b) == y[i]:\n",
    "            correct += 1\n",
    "        \n",
    "            \n",
    "    return (correct/m)*100\n",
    "\n",
    "\n",
    "\n",
    "def gradient_descent(X, Y, W, b, learning_rate, num_iterations):\n",
    "    J_history = []\n",
    "    for i in range(num_iterations):\n",
    "        \n",
    "        cost,dW,db=compute_gradient(X, Y, W, b)\n",
    "        W=W-learning_rate*dW\n",
    "        b=b-learning_rate*db\n",
    "        \n",
    "        if i% math.ceil(num_iterations / 10) == 0:\n",
    "                print(f\"Iteration {i:4d}: Cost {cost}   \")\n",
    "                print(f\"Accuracy on training set: {accuracy(X, Y, W, b)}%\")\n",
    "    return W,b\n",
    "\n",
    "W,b=gradient_descent(X_train, y_train, W, b, 0.5, 2000)\n",
    "\n",
    "\n",
    "np.savez('model.npz', W=W, b=b, features_list=features_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0: Cost 1.2326203413764087   \n",
      "Accuracy on training set: 14.6218487394958%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 86\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy on training set: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy(X,\u001b[38;5;250m \u001b[39mY,\u001b[38;5;250m \u001b[39mW,\u001b[38;5;250m \u001b[39mb)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m W,b\n\u001b[0;32m---> 86\u001b[0m W,b\u001b[38;5;241m=\u001b[39m\u001b[43mgradient_descent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m np\u001b[38;5;241m.\u001b[39msavez(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel.npz\u001b[39m\u001b[38;5;124m'\u001b[39m, W\u001b[38;5;241m=\u001b[39mW, b\u001b[38;5;241m=\u001b[39mb, features_list\u001b[38;5;241m=\u001b[39mfeatures_list)\n",
      "Cell \u001b[0;32mIn[2], line 77\u001b[0m, in \u001b[0;36mgradient_descent\u001b[0;34m(X, Y, W, b, learning_rate, num_iterations)\u001b[0m\n\u001b[1;32m     74\u001b[0m J_history \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_iterations):\n\u001b[0;32m---> 77\u001b[0m     cost,dW,db\u001b[38;5;241m=\u001b[39m\u001b[43mcompute_gradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m     W\u001b[38;5;241m=\u001b[39mW\u001b[38;5;241m-\u001b[39mlearning_rate\u001b[38;5;241m*\u001b[39mdW\n\u001b[1;32m     79\u001b[0m     b\u001b[38;5;241m=\u001b[39mb\u001b[38;5;241m-\u001b[39mlearning_rate\u001b[38;5;241m*\u001b[39mdb\n",
      "Cell \u001b[0;32mIn[2], line 35\u001b[0m, in \u001b[0;36mcompute_gradient\u001b[0;34m(X, Y, W, b)\u001b[0m\n\u001b[1;32m     31\u001b[0m loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.\u001b[39m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(m):\n\u001b[0;32m---> 35\u001b[0m     A\u001b[38;5;241m=\u001b[39m(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mW\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m+\u001b[39mb)\n\u001b[1;32m     37\u001b[0m     loss\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39mY[i]\u001b[38;5;241m*\u001b[39mnp\u001b[38;5;241m.\u001b[39mlog(sigmoid(A[\u001b[38;5;241m0\u001b[39m]))\u001b[38;5;241m+\u001b[39m(\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39mY[i])\u001b[38;5;241m*\u001b[39mnp\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39msigmoid(A[\u001b[38;5;241m0\u001b[39m]))\n\u001b[1;32m     38\u001b[0m     Z\u001b[38;5;241m=\u001b[39msigmoid(A[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pickle\n",
    "import math\n",
    "from featureSelection import features_selection\n",
    "\n",
    "with open('data_train.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "    \n",
    "features_list=[0,1,2]\n",
    "X_train, X_val, y_train, y_val = features_selection(features_list)\n",
    "\n",
    "def sigmoid(Z):\n",
    "    return 1/(1+np.exp(-Z))\n",
    "\n",
    "\n",
    "def initialize_parameters(dim):\n",
    "    W = np.random.rand(dim, 1)\n",
    "    b = 0\n",
    "    return W, b\n",
    "\n",
    "W,b = initialize_parameters(X_train.shape[1])\n",
    "\n",
    "\n",
    "\n",
    "def compute_gradient(X, Y, W, b):\n",
    "    \n",
    "\n",
    "    m,n = X.shape\n",
    "    dj_dw = np.zeros((n,))                           #(n,)\n",
    "    dj_db = 0.\n",
    "    loss=0.\n",
    "    \n",
    "    \n",
    "    for i in range(m):\n",
    "        A=(np.dot(X[i],W)+b)\n",
    "       \n",
    "        loss+=Y[i]*np.log(sigmoid(A[0]))+(1-Y[i])*np.log(1-sigmoid(A[0]))\n",
    "        Z=sigmoid(A[0])\n",
    "        for j in range(n):\n",
    "            dj_dw[j]=dj_dw[j]+(Z-Y[i])*X[i,j]\n",
    "        dj_db+=(Z-Y[i])\n",
    "        dj_dw = dj_dw/m                                   #(n,)\n",
    "        dj_db = dj_db/m  \n",
    "        \n",
    "    cost=-loss/m\n",
    "        \n",
    "    \n",
    "    return cost,dj_dw, dj_db\n",
    "\n",
    "\n",
    "def predict(X_i, W,  b,threshold=0.5):\n",
    "    A=(np.dot(X_i,W)+b)\n",
    "    Z=  sigmoid(A[0])\n",
    "    if Z >threshold:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "\n",
    "def accuracy(X, y, W, b):\n",
    "    \n",
    "    m = X.shape[0]\n",
    "    correct = 0\n",
    "    for i in range(m):\n",
    "        if predict(X[i], W, b) == y[i]:\n",
    "            correct += 1\n",
    "        \n",
    "            \n",
    "    return (correct/m)*100\n",
    "\n",
    "\n",
    "\n",
    "def gradient_descent(X, Y, W, b, learning_rate, num_iterations):\n",
    "    J_history = []\n",
    "    for i in range(num_iterations):\n",
    "        \n",
    "        cost,dW,db=compute_gradient(X, Y, W, b)\n",
    "        W=W-learning_rate*dW\n",
    "        b=b-learning_rate*db\n",
    "        \n",
    "        if i% math.ceil(num_iterations / 10) == 0:\n",
    "                print(f\"Iteration {i:4d}: Cost {cost}   \")\n",
    "                print(f\"Accuracy on training set: {accuracy(X, Y, W, b)}%\")\n",
    "    return W,b\n",
    "\n",
    "W,b=gradient_descent(X_train, y_train, W, b, 0.5, 2000)\n",
    "\n",
    "\n",
    "np.savez('model.npz', W=W, b=b, features_list=features_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.705882352941176\n"
     ]
    }
   ],
   "source": [
    "#TEST CELL, DONT TOUCH UNTIL DONE\n",
    "import pickle \n",
    "import numpy as np\n",
    "from featureSelection import features_selection_test_set\n",
    "\n",
    "\n",
    "model=np.load('model.npz')\n",
    "\n",
    "W=model['W']\n",
    "b=model['b']\n",
    "features_list=model['features_list']\n",
    "\n",
    "\n",
    "X_test, y_test = features_selection_test_set(features_list)\n",
    "\n",
    "#RANDOMIZED FOR TESTING PURPOSE, WILL BE IMPORTED FROM TRAINING SOON\n",
    "\n",
    "\n",
    "def sigmoid(X):\n",
    "    return 1/(1+np.exp(-X))\n",
    "\n",
    "\n",
    "def predict(X_i, W,  b,threshold=0.5):\n",
    "    A=(np.dot(X_i,W)+b)\n",
    "    Z=  sigmoid(A[0])\n",
    "    if Z >threshold:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "\n",
    "def accuracy(X, y, W, b):\n",
    "    \n",
    "    m = X.shape[0]\n",
    "    correct = 0\n",
    "    for i in range(m):\n",
    "        if predict(X[i], W, b) == y[i]:\n",
    "            correct += 1\n",
    "        \n",
    "            \n",
    "    return (correct/m)*100\n",
    "\n",
    "\n",
    "acc=accuracy(X_test, y_test, W, b)\n",
    "print(acc)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
