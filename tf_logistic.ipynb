{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2975, 55)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "dot() missing 1 required positional argument: 'b'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 68\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIteration \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m4d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: Cost \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcost\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m   \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m W,b\n\u001b[0;32m---> 68\u001b[0m W,b\u001b[38;5;241m=\u001b[39m\u001b[43mgradient_descent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m np\u001b[38;5;241m.\u001b[39msavez(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel.npz\u001b[39m\u001b[38;5;124m'\u001b[39m, W\u001b[38;5;241m=\u001b[39mW, b\u001b[38;5;241m=\u001b[39mb, features_list\u001b[38;5;241m=\u001b[39mfeatures_list)\n",
      "Cell \u001b[0;32mIn[11], line 60\u001b[0m, in \u001b[0;36mgradient_descent\u001b[0;34m(X, Y, W, b, learning_rate, num_iterations)\u001b[0m\n\u001b[1;32m     57\u001b[0m J_history \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_iterations):\n\u001b[0;32m---> 60\u001b[0m     cost,dW,db\u001b[38;5;241m=\u001b[39m\u001b[43mcompute_gradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m     W\u001b[38;5;241m=\u001b[39mW\u001b[38;5;241m-\u001b[39mlearning_rate\u001b[38;5;241m*\u001b[39mdW\n\u001b[1;32m     62\u001b[0m     b\u001b[38;5;241m=\u001b[39mb\u001b[38;5;241m-\u001b[39mlearning_rate\u001b[38;5;241m*\u001b[39mdb\n",
      "Cell \u001b[0;32mIn[11], line 37\u001b[0m, in \u001b[0;36mcompute_gradient\u001b[0;34m(X, Y, W, b)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(m):\n\u001b[1;32m     36\u001b[0m     A\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.\u001b[39m\n\u001b[0;32m---> 37\u001b[0m     \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mW\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mA\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m     A\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39mb       \n\u001b[1;32m     39\u001b[0m     loss\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39mY[i]\u001b[38;5;241m*\u001b[39mnp\u001b[38;5;241m.\u001b[39mlog(sigmoid(A[\u001b[38;5;241m0\u001b[39m]))\u001b[38;5;241m+\u001b[39m(\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39mY[i])\u001b[38;5;241m*\u001b[39mnp\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39msigmoid(A[\u001b[38;5;241m0\u001b[39m]))\n",
      "\u001b[0;31mTypeError\u001b[0m: dot() missing 1 required positional argument: 'b'"
     ]
    }
   ],
   "source": [
    "import cupy as np \n",
    "import numpy as nnp\n",
    "import pickle\n",
    "import math\n",
    "from featureSelection import features_selection\n",
    "\n",
    "with open('data_train.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "    \n",
    "features_list=[0,1,2]\n",
    "X_train, X_val, y_train, y_val = features_selection(features_list)\n",
    "\n",
    "def sigmoid(Z):\n",
    "    return 1/(1+np.exp(-Z))\n",
    "\n",
    "\n",
    "def initialize_parameters(dim):\n",
    "    W = np.random.rand(dim, 1)\n",
    "    b = 0\n",
    "    return W, b\n",
    "\n",
    "W,b = initialize_parameters(X_train.shape[1])\n",
    "X_train = X_train.astype(np.float32)\n",
    "\n",
    "print(X_train.shape)\n",
    "def compute_gradient(X, Y, W, b):\n",
    "    \n",
    "\n",
    "    m,n = X.shape\n",
    "    dj_dw = np.zeros((n,))                           #(n,)\n",
    "    dj_db = 0.\n",
    "    loss=0.\n",
    "    \n",
    "    \n",
    "    for i in range(m):\n",
    "        A=0.\n",
    "        np.dot((X[i],W),out=A)\n",
    "        A+=b       \n",
    "        loss+=Y[i]*np.log(sigmoid(A[0]))+(1-Y[i])*np.log(1-sigmoid(A[0]))\n",
    "        Z=sigmoid(A[0])\n",
    "        for j in range(n):\n",
    "            dj_dw[j]=dj_dw[j]+(Z-Y[i])*X[i,j]\n",
    "        dj_db+=(Z-Y[i])\n",
    "        dj_dw = dj_dw/m                                   #(n,)\n",
    "        dj_db = dj_db/m  \n",
    "        \n",
    "    cost=-loss/m\n",
    "        \n",
    "    \n",
    "    return cost,dj_dw, dj_db\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def gradient_descent(X, Y, W, b, learning_rate, num_iterations):\n",
    "    J_history = []\n",
    "    for i in range(num_iterations):\n",
    "        \n",
    "        cost,dW,db=compute_gradient(X, Y, W, b)\n",
    "        W=W-learning_rate*dW\n",
    "        b=b-learning_rate*db\n",
    "        \n",
    "        if i% math.ceil(num_iterations / 10) == 0:\n",
    "                print(f\"Iteration {i:4d}: Cost {cost}   \")\n",
    "    return W,b\n",
    "\n",
    "W,b=gradient_descent(X_train, y_train, W, b, 0.5, 2000)\n",
    "\n",
    "\n",
    "np.savez('model.npz', W=W, b=b, features_list=features_list)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
