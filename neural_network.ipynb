{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[107 0 1 26 161.6 123 27.47 195.5 103 16.62 254.4 103 11.45 13.7 3 3.7 1 0\n",
      " 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "(1492,)\n",
      "(2975, 71)\n",
      "[0 1 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "df =pd.read_csv('train.csv')\n",
    "\n",
    "### UNCOMMENT THIS PART TO USE THE FEATURE ENGINEERING\n",
    "# df['total_call'] = df['total_day_calls'] + df['total_eve_calls'] + df['total_night_calls']\n",
    "\n",
    "# # Create 'total_charges' feature\n",
    "# df['total_charges'] = df['total_day_charge'] + df['total_eve_charge'] + df['total_night_charge']\n",
    "\n",
    "# # Create 'total_minutes' feature\n",
    "# df['total_minutes'] = df['total_day_minutes'] + df['total_eve_minutes'] + df['total_night_minutes']\n",
    "# df = df.drop(['total_day_calls', 'total_eve_calls', 'total_night_calls'], axis=1)\n",
    "\n",
    "# # Delete contributing features for 'total_charges'\n",
    "# df = df.drop(['total_day_charge', 'total_eve_charge', 'total_night_charge'], axis=1)\n",
    "\n",
    "# # Delete contributing features for 'total_minutes'\n",
    "# df = df.drop(['total_day_minutes', 'total_eve_minutes', 'total_night_minutes'], axis=1)\n",
    "\n",
    "\n",
    "# df.drop(['state', 'area_code', 'account_length'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "###################\n",
    "\n",
    "###ONE HOT ENCODING\n",
    "df = pd.get_dummies(df, columns=['area_code','state'])\n",
    "\n",
    "\n",
    "### MOVING THE Y VARIABLE TO THE END\n",
    "churn = df['churn']\n",
    "df = df.drop('churn', axis=1)\n",
    "df['churn'] = churn\n",
    "\n",
    "\n",
    "data=np.array(df)\n",
    "\n",
    "\n",
    "data[data=='no']=0\n",
    "data[data=='yes']=1\n",
    "data[data==False]=0\n",
    "data[data==True]=1\n",
    "print(data[0])\n",
    "X=data[:,:-1]\n",
    "y=data[:,-1]\n",
    "\n",
    "\n",
    "\n",
    "### SPLITTING THE DATA INTO TRAIN, VALIDATION AND TEST SETS\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42) \n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42) \n",
    "\n",
    "###DATA NORMALIZATION\n",
    "def normalize(X):\n",
    "    X = X.astype(float)\n",
    "    X=(X-X.mean(axis=0))/X.std(axis=0)\n",
    "    return X\n",
    "X_train = normalize(X_train)\n",
    "X_val = normalize(X_val)\n",
    "X_test = normalize(X_test)\n",
    "\n",
    "#SMOTE, oversampling the minority class (will read more about this later)\n",
    "X_train_oversampled_smote = []\n",
    "labels_train_oversampled_smote = []\n",
    "indices_0 = np.where(y_train == 0)[0]\n",
    "indices_1 = np.where(y_train == 1)[0]\n",
    "indices = np.concatenate([indices_0, indices_1])\n",
    "for _ in range(X_train.shape[0]):\n",
    "    p = np.random.random()\n",
    "    #sample from majority class\n",
    "    if p < 0.5:\n",
    "        X_train_oversampled_smote.append(X_train[np.random.choice(indices_0)])\n",
    "        labels_train_oversampled_smote.append(0)\n",
    "    #sample from minority class\n",
    "    else:\n",
    "        #get two random samples from minority class\n",
    "        minority_samp_1 = X_train[np.random.choice(indices_1)]\n",
    "        minority_samp_2 = X_train[np.random.choice(indices_1)]\n",
    "        \n",
    "        #get random proportion with which to mix them\n",
    "        prop = np.random.random()\n",
    "        \n",
    "        #generate synthetic sample from minority class\n",
    "        synthetic_minority_samp = prop*minority_samp_1 + (1-prop)*minority_samp_2\n",
    "        X_train_oversampled_smote.append(synthetic_minority_samp)\n",
    "        labels_train_oversampled_smote.append(1)\n",
    "        \n",
    "X_train = np.array(X_train_oversampled_smote)\n",
    "y_train = np.array(labels_train_oversampled_smote)\n",
    "\n",
    "print(y_train[y_train==0].shape)\n",
    "print(X_train.shape)\n",
    "\n",
    "####################\n",
    "\n",
    "print(y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2974, Cost: 24.645447030102986\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (2975,) and (10,) not aligned: 2975 (dim 0) != 10 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 88\u001b[0m\n\u001b[1;32m     84\u001b[0m         weights, biases \u001b[38;5;241m=\u001b[39m update_parameters(weights, biases, dW, db, learning_rate)\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m weights, biases\n\u001b[0;32m---> 88\u001b[0m weights, biases \u001b[38;5;241m=\u001b[39m \u001b[43mgradient_descent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m    \n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(X, layers, weights, biases):\n\u001b[1;32m     91\u001b[0m     y_hat \u001b[38;5;241m=\u001b[39m forward_propagation(X, layers, weights, biases)\n",
      "Cell \u001b[0;32mIn[21], line 83\u001b[0m, in \u001b[0;36mgradient_descent\u001b[0;34m(X_train, y_train, layers, learning_rate, epochs)\u001b[0m\n\u001b[1;32m     81\u001b[0m     cost \u001b[38;5;241m=\u001b[39m compute_cost(y_train, y_hat)\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Cost: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcost\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 83\u001b[0m     dW, db \u001b[38;5;241m=\u001b[39m \u001b[43mback_propagation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_hat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbiases\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m     weights, biases \u001b[38;5;241m=\u001b[39m update_parameters(weights, biases, dW, db, learning_rate)\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m weights, biases\n",
      "Cell \u001b[0;32mIn[21], line 59\u001b[0m, in \u001b[0;36mback_propagation\u001b[0;34m(X, y, y_hat, layers, weights, biases)\u001b[0m\n\u001b[1;32m     57\u001b[0m dC_dA \u001b[38;5;241m=\u001b[39m (y_hat \u001b[38;5;241m-\u001b[39m y)\n\u001b[1;32m     58\u001b[0m dC_dZ \u001b[38;5;241m=\u001b[39m dC_dA\u001b[38;5;241m.\u001b[39mdot(reLU_derivative(Activation_list[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]))\n\u001b[0;32m---> 59\u001b[0m dZ_dW \u001b[38;5;241m=\u001b[39m \u001b[43mdC_dZ\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mActivation_list\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m dZ_db \u001b[38;5;241m=\u001b[39m dC_dZ\n\u001b[1;32m     61\u001b[0m dW\u001b[38;5;241m.\u001b[39mappend(dZ_dW)\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (2975,) and (10,) not aligned: 2975 (dim 0) != 10 (dim 0)"
     ]
    }
   ],
   "source": [
    "def reLU(z):\n",
    "    return max(0,z)\n",
    "\n",
    "def reLU_derivative(z):\n",
    "    return 1 if z>0 else 0\n",
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "\n",
    "def sigmoid_derivative(z):\n",
    "    return sigmoid(z)*(1-sigmoid(z))\n",
    "\n",
    "def dense_layer(A_in, W, b):\n",
    "     units = W.shape[1]\n",
    "     A_out = np.zeros(units)\n",
    "     for j in range(units):               \n",
    "            w = W[:,j]                                    \n",
    "            z = np.dot(w, A_in) + b[j]         \n",
    "            A_out[j] = reLU(z)               \n",
    "     return(A_out)\n",
    " \n",
    " \n",
    "layers =[X_train.shape[1], 10, 10, 1]\n",
    "\n",
    "\n",
    "\n",
    "def initialize_parameters(layers):\n",
    "    weights = []\n",
    "    biases = []\n",
    "    for i in range(1, len(layers)):\n",
    "        weights.append(np.random.randn(layers[i-1], layers[i]))\n",
    "        biases.append(np.random.randn(layers[i]))\n",
    "    return weights, biases\n",
    "\n",
    "Activation_list=[]\n",
    "def forward_propagation(X, layers, weights, biases):\n",
    "    A = X\n",
    "    for i in range(len(layers)-1):\n",
    "        A = dense_layer(A, weights[i], biases[i])\n",
    "        Activation_list.append(A)\n",
    "    return A\n",
    "\n",
    "\n",
    "def update_parameters(weights, biases, dW, db, learning_rate):\n",
    "    for i in range(len(weights)):\n",
    "        weights[i] = weights[i] - learning_rate*dW[i]\n",
    "        biases[i] = biases[i] - learning_rate*db[i]\n",
    "    return weights, biases\n",
    "\n",
    "def compute_cost(y, y_hat):\n",
    "    return np.mean((y-y_hat)**2)\n",
    "\n",
    "def back_propagation(X, y, y_hat, layers, weights, biases):\n",
    "    dW = []\n",
    "    db = []\n",
    "    m = X.shape[0]\n",
    "    dC_dA = (y_hat - y)\n",
    "    dC_dZ = dC_dA.dot(reLU_derivative(Activation_list[-1]))\n",
    "    dZ_dW = dC_dZ.dot(Activation_list[-2])\n",
    "    dZ_db = dC_dZ\n",
    "    dW.append(dZ_dW)\n",
    "    db.append(dZ_db)\n",
    "    for i in range(len(layers)-2, 0, -1):\n",
    "        dC_dA = dC_dZ.dot(weights[i].T)\n",
    "        dC_dZ = dC_dA*reLU_derivative(Activation_list[i-1])\n",
    "        dZ_dW = dC_dZ.dot(Activation_list[i-1].T)\n",
    "        dZ_db = dC_dZ\n",
    "        dW.append(dZ_dW)\n",
    "        db.append(dZ_db)\n",
    "    return dW, db\n",
    "    \n",
    "    \n",
    "    \n",
    "def gradient_descent(X_train, y_train, layers, learning_rate, epochs):\n",
    "    weights, biases = initialize_parameters(layers)\n",
    "    for i in range(epochs):\n",
    "        y_hat=np.zeros(y_train.shape)\n",
    "        for i in range(X_train.shape[0]):\n",
    "            y_hat[i] = forward_propagation(X_train[i], layers, weights, biases)\n",
    "        \n",
    "        cost = compute_cost(y_train, y_hat)\n",
    "        print(f'Epoch {i}, Cost: {cost}')\n",
    "        dW, db = back_propagation(X_train, y_train, y_hat, layers, weights, biases)\n",
    "        weights, biases = update_parameters(weights, biases, dW, db, learning_rate)\n",
    "        \n",
    "    return weights, biases\n",
    "\n",
    "weights, biases = gradient_descent(X_train, y_train, layers, 0.01, 1000)    \n",
    "    \n",
    "def predict(X, layers, weights, biases):\n",
    "    y_hat = forward_propagation(X, layers, weights, biases)\n",
    "    return y_hat\n",
    "\n",
    "y_hat = predict(X_val, layers, weights, biases)\n",
    "y_hat = np.round(y_hat)\n",
    "print(y_hat)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3, -3, -3])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
